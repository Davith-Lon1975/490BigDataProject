# -*- coding: utf-8 -*-
"""Depression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TGJ_Ridl4oerveJXScOKbbUAPQtNMAs6
"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q http://apache.osuosl.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz
!tar xf spark-3.1.1-bin-hadoop2.7.tgz
!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.1.1-bin-hadoop2.7"

import findspark
import string
findspark.init()
from pyspark.sql import SparkSession
from pyspark import SparkConf
from pyspark import SparkContext

spark = SparkSession.builder.appName("Depression").getOrCreate()

sc = spark.sparkContext

from google.colab import drive
drive.mount('/content/drive')

df = spark.read.option("header", "true").option("multiLine", "true").option("quote", "\"").option("escape", "\"").option("inferSchema", "true").csv('/content/drive/MyDrive/490/Cleaned_Depression_Vs_Suicide.csv')
df.show()

totale_rows = df.count() # The total amount of posts we have 
df.count()

post_lengths = df.rdd.map(lambda x: len(x.text))
post_lengths.take(10) # Show the lengths of the first 10 posts

print(post_lengths.stats())

texts = df.rdd.map(lambda x : x.text)
texts.take(5)

def lower_clean_str(x):
  lowercased_str = x.lower()
  for ch in string.punctuation:
    lowercased_str = lowercased_str.replace(ch, '')
  return lowercased_str

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

stop = stopwords.words("english")

print(stop)

print('a' in stop)

def filter(word):
   if word not in stop:
    return word

words = texts.flatMap( lambda post: post.split(" ")).map(lambda x: filter(x)).filter(lambda x: x is not None).filter(lambda x: x != "")
words.take(5)

word_count = words.map( lambda word: (word, 1)).reduceByKey(lambda x, y: x + y).sortBy(lambda pair: pair[1], ascending = False)
word_count.take(10)



from wordcloud import WordCloud
import matplotlib.pyplot as plt

word_cloud_df = spark.createDataFrame(word_count)

word_cloud_df.show()

import numpy
import pandas

def wordcloud(corpus_sdf):
    corpus_pdf = corpus_sdf.limit(500).toPandas()

    corpus_dict = {}
    for index, row in corpus_pdf.iterrows():
        corpus_dict[row['_1']] = row['_2']
        
    wordcloud = WordCloud().generate_from_frequencies(corpus_dict)
    plt.imshow(wordcloud);

wordcloud(word_cloud_df)