{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MyRedditScrape.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8_dYoGLv0LD"
      },
      "source": [
        "# PySpark SetUp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5H19BuQvsWn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c2b666e-3d47-4c40-8ea9-51ebc89d428a"
      },
      "source": [
        "!apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.osuosl.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.18\u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.18\u001b[0m\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.18\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com] [Connecting to\u001b[0m\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com] [Connecting to\u001b[0m\r                                                                               \rGet:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com] [Connecting to\u001b[0m\r                                                                               \rHit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:10 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [55.5 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:13 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Ign:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [770 kB]\n",
            "Hit:15 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:17 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,759 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,550 kB]\n",
            "Get:20 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [900 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,182 kB]\n",
            "Get:22 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [49.5 kB]\n",
            "Fetched 8,561 kB in 4s (1,990 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "54 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2x4qavL5JS-t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f973cd8-43e2-46cd-e240-f818126340b5"
      },
      "source": [
        "!pip install afinn"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: afinn in /usr/local/lib/python3.7/dist-packages (0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGZ6JIvZvzSd"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop2.7\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6I6TGaCv2zW"
      },
      "source": [
        "import findspark\n",
        "import string\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import StringType, IntegerType\n",
        "import string\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import HashingTF, StopWordsRemover, Tokenizer, IDF\n",
        "from pyspark.ml.classification import  NaiveBayes, LogisticRegression, LinearSVC\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "import re\n",
        "from afinn import Afinn\n",
        "import pandas\n",
        "\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.appName(\"MyRedditScrape\").getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "sqlContext = SQLContext(sc)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqJYTTKsB8cA"
      },
      "source": [
        "# Loading the Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tl-lnLvMB76T"
      },
      "source": [
        "df = spark.read.option(\"header\", \"false\").option(\"quote\", \"\\\"\").option(\"escape\", \"\\\"\").option(\"inferSchema\", \"true\").csv(\"/content/drive/MyDrive/490/Data/cleanData.csv\").toDF(\"selftext\", \"subreddit\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bSBfHTFCLuA",
        "outputId": "4678e395-3bf8-4af5-a5b1-512e9c0e1ef3"
      },
      "source": [
        "# Take look and make sure everything is ok\n",
        "df.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+----------+\n",
            "|            selftext| subreddit|\n",
            "+--------------------+----------+\n",
            "|ive had depressio...|depression|\n",
            "|i just need to ve...|depression|\n",
            "|i see the world s...|depression|\n",
            "|hey reddit i hope...|depression|\n",
            "|ive sought advice...|depression|\n",
            "|does anyone else ...|depression|\n",
            "|so a while ago i ...|depression|\n",
            "|i get so anxious ...|depression|\n",
            "|i recently  came ...|depression|\n",
            "|im a   36 male fr...|depression|\n",
            "|well im here to a...|depression|\n",
            "|ive always felt l...|depression|\n",
            "|is it bad that if...|depression|\n",
            "|let me preface th...|depression|\n",
            "|i just cant i tri...|depression|\n",
            "|ive been done for...|depression|\n",
            "|im 26 and just we...|depression|\n",
            "|a lot of the time...|depression|\n",
            "|it feels like no ...|depression|\n",
            "|so i wake up in a...|depression|\n",
            "+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiXGZPJU_fpV"
      },
      "source": [
        "# Labeling (AFINN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05nz2Dpn_hog"
      },
      "source": [
        "# Create out Afinn object\n",
        "afin = Afinn(language='en')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RrJqXnRKBSM",
        "outputId": "c01ef759-a5cd-4a27-9af6-057b6345f427"
      },
      "source": [
        "afin.score(\"I want to die\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-2.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBYagTBvyEkE",
        "outputId": "a35b0d9d-4108-471c-c0aa-0bbf4d1a5616"
      },
      "source": [
        "afin.score(\"I love the world\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsLmZDidV8F2",
        "outputId": "269174dc-7ff5-41bb-9f4b-7042579f4e74"
      },
      "source": [
        "type(afin.score(\"I want to die\"))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "float"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLGcNEXiv1ep"
      },
      "source": [
        "# This Function will return 1 if the text is negative and 0 if the text is positive.\n",
        "# This is based on the scoring from the Afinn object\n",
        "udfNew = F.udf(lambda x: 1 if afin.score(x) < 0 else 0)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ief1zxD667G"
      },
      "source": [
        "data = df.select(F.col('selftext'), udfNew(F.col('selftext')).alias('label'))\n",
        "data = data.withColumn(\"label\", F.col(\"label\").cast(\"int\"))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lO3e5CQ6OqOK",
        "outputId": "dbea35c7-6c06-470e-db3f-6553f950ef9f"
      },
      "source": [
        "data.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-----+\n",
            "|            selftext|label|\n",
            "+--------------------+-----+\n",
            "|ive had depressio...|    1|\n",
            "|i just need to ve...|    1|\n",
            "|i see the world s...|    1|\n",
            "|hey reddit i hope...|    0|\n",
            "|ive sought advice...|    1|\n",
            "|does anyone else ...|    1|\n",
            "|so a while ago i ...|    1|\n",
            "|i get so anxious ...|    1|\n",
            "|i recently  came ...|    1|\n",
            "|im a   36 male fr...|    1|\n",
            "|well im here to a...|    1|\n",
            "|ive always felt l...|    1|\n",
            "|is it bad that if...|    1|\n",
            "|let me preface th...|    1|\n",
            "|i just cant i tri...|    0|\n",
            "|ive been done for...|    1|\n",
            "|im 26 and just we...|    1|\n",
            "|a lot of the time...|    0|\n",
            "|it feels like no ...|    1|\n",
            "|so i wake up in a...|    1|\n",
            "+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDvIS_BTSG_o"
      },
      "source": [
        "data.registerTempTable(\"dataWithLabel\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxZ3WdfjSNQv"
      },
      "source": [
        "sqlContext.sql(\"SELECT label, COUNT(*) as count from dataWithLabel GROUP BY label\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Eu9sDTKXFhH"
      },
      "source": [
        "# Even Out the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1CqNSLLXFGO"
      },
      "source": [
        "temp1 = sqlContext.sql(\"SELECT * from dataWithLabel WHERE label = 1 LIMIT 50000\")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qStUlinnfdNw"
      },
      "source": [
        "temp2 = sqlContext.sql(\"SELECT * from dataWithLabel WHERE label = 0 LIMIT 50000\")"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtQ5YQyxfp6v"
      },
      "source": [
        "data2 = temp1.union(temp2)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6yAvpPPgIHR"
      },
      "source": [
        "data2 = data2.withColumn(\"label\", data2[\"label\"].cast(IntegerType()))\n",
        "data2 = data2.withColumn(\"selftext\", data2[\"selftext\"].cast(StringType()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpz9Aw8sE8ii"
      },
      "source": [
        "# Sample our Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXkEZq5fE-h3"
      },
      "source": [
        "# We take a 10% Sample of our data\n",
        "data = data2.sample(False, 0.01)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3kvbXQnyZGs"
      },
      "source": [
        "# Set Up Elements in Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVe4UDedyavX"
      },
      "source": [
        "tokenizer = Tokenizer(inputCol=\"selftext\", outputCol=\"words\")"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfW1RlKpzrDL"
      },
      "source": [
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", caseSensitive=False)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1xwECge0E7c"
      },
      "source": [
        "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawfeatures\", numFeatures= 4096)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBgbvtTI0X-4"
      },
      "source": [
        "idf = IDF(inputCol=\"rawfeatures\", outputCol=\"features\", minDocFreq= 0)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNp72clZIoLm"
      },
      "source": [
        "lr = LogisticRegression(regParam=0.01, threshold=0.5)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIfeWSqq2cx8"
      },
      "source": [
        "nb = NaiveBayes()"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqBPpIWFvJ0c"
      },
      "source": [
        "lsvc = LinearSVC(regParam= 0.01, threshold=0.5)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbPtTDEP1bq7"
      },
      "source": [
        "pipeline1 = Pipeline(stages=[tokenizer, remover, hashingTF, idf, lr])"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Gn0zdq0dEQP"
      },
      "source": [
        "pipeline2 = Pipeline(stages=[tokenizer, remover, hashingTF, idf, nb])"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdr1may6dJtM"
      },
      "source": [
        " pipeline3 = Pipeline(stages=[tokenizer, remover, hashingTF, idf, lsvc])"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2RbjTtD9ybV"
      },
      "source": [
        "# Test Train Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulXOjdND90IX"
      },
      "source": [
        "# splits[0] is my training set, splits[1] is my testing set\n",
        "splits = data.randomSplit([0.9, 0.1], 1234)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iMz2Y4Ga5Pu"
      },
      "source": [
        "#Modeling and Predicting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KG3sMnq_ZTy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 800
        },
        "outputId": "3e8a8dd9-9b9c-4dea-e02a-2282607bc875"
      },
      "source": [
        "# Logistic Regression Model\n",
        "model1 = pipeline1.fit(splits[0])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PythonException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-af71a5d8897e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Logistic Regression Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/content/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/content/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/content/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/content/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n    for obj in iterator:\n  File \"/content/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n    for item in iterator:\n  File \"/content/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/content/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/content/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/content/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-12-1b5fe0a2573e>\", line 3, in <lambda>\n  File \"/usr/local/lib/python3.7/dist-packages/afinn/afinn.py\", line 335, in score_with_pattern\n    word_scores = self.scores_with_pattern(text)\n  File \"/usr/local/lib/python3.7/dist-packages/afinn/afinn.py\", line 369, in scores_with_pattern\n    words = self.find_all(text)\n  File \"/usr/local/lib/python3.7/dist-packages/afinn/afinn.py\", line 288, in find_all\n    text = re.sub(r\"\\s+\", \" \", text)\n  File \"/usr/lib/python3.7/re.py\", line 194, in sub\n    return _compile(pattern, flags).sub(repl, string, count)\nTypeError: expected string or bytes-like object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmR6-7bRcC98"
      },
      "source": [
        "# Naive Bayes Model\n",
        "model2 = pipeline2.fit(splits[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2m56GNBcFTg"
      },
      "source": [
        "# Linear Support Vector Classification Model\n",
        "model3 = pipeline3.fit(splits[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYK7_ZdUgBXe"
      },
      "source": [
        "# Logistic Regression Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99hcrY77ElBv"
      },
      "source": [
        "predictions1 = model1.transform(splits[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cyq3oJhpE0OH"
      },
      "source": [
        "predictions1.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Akf68pZxTQgH"
      },
      "source": [
        "# Binary Classification Evaluator\n",
        "\n",
        "\n",
        "eval1 = BinaryClassificationEvaluator(metricName=\"areaUnderROC\")\n",
        "print(\"Area Under the ROC Curve: {}\".format(eval1.evaluate(predictions1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mG-Ghp8ndEpn"
      },
      "source": [
        "# Multiclass Classification Evaluator\n",
        "\n",
        "\n",
        "eval2 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "print(\"Accuracy: \" + str(eval2.evaluate(predictions1)))\n",
        "\n",
        "eval3 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
        "\n",
        "print(\"Precision: \" + str(eval2.evaluate(predictions1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fj9cNp5wgVdE"
      },
      "source": [
        "# Naives Bayes Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38T74ZOohDIL"
      },
      "source": [
        "predictions2 = model2.transform(splits[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Apo1UFbhC4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d636f693-c74d-4cd5-91a0-25a236e9722b"
      },
      "source": [
        "predictions2.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|            selftext|label|               words|            filtered|         rawfeatures|            features|       rawPrediction|         probability|prediction|\n",
            "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|17 days now shes ...|    0|[17, days, now, s...|[17, days, shes, ...|(4096,[122,125,63...|(4096,[122,125,63...|[-400.69183906900...|[0.99681312466661...|       0.0|\n",
            "|175 high school s...|    0|[175, high, schoo...|[175, high, schoo...|(4096,[78,131,163...|(4096,[78,131,163...|[-2901.4526493752...|[0.99439078012153...|       0.0|\n",
            "|21 year old male ...|    1|[21, year, old, m...|[21, year, old, m...|(4096,[32,109,120...|(4096,[32,109,120...|[-1905.1010826250...|[0.02692982929418...|       1.0|\n",
            "|22yo male and pre...|    1|[22yo, male, and,...|[22yo, male, pret...|(4096,[32,131,148...|(4096,[32,131,148...|[-2759.3525455638...|[0.02260647931333...|       1.0|\n",
            "|25 years old  - n...|    1|[25, years, old, ...|[25, years, old, ...|(4096,[16,18,32,5...|(4096,[16,18,32,5...|[-5352.6698204496...|[1.95029296885232...|       1.0|\n",
            "|27m posting with ...|    1|[27m, posting, wi...|[27m, posting, th...|(4096,[83,107,109...|(4096,[83,107,109...|[-5746.7819026451...|[0.06593046687607...|       1.0|\n",
            "|33m i have litera...|    1|[33m, i, have, li...|[33m, literally, ...|(4096,[237,379,40...|(4096,[237,379,40...|[-653.51203675599...|[0.18241287397685...|       1.0|\n",
            "|4 years now since...|    1|[4, years, now, s...|[4, years, since,...|(4096,[122,144,17...|(4096,[122,144,17...|[-2909.9928877981...|[0.00424367398692...|       1.0|\n",
            "|5 months ago my l...|    0|[5, months, ago, ...|[5, months, ago, ...|(4096,[27,97,109,...|(4096,[27,97,109,...|[-2422.1552272463...|[0.99999995098125...|       0.0|\n",
            "|5 years ago i spe...|    0|[5, years, ago, i...|[5, years, ago, s...|(4096,[51,68,131,...|(4096,[51,68,131,...|[-1254.5313315385...|[0.99997254525880...|       0.0|\n",
            "|\\dm for link\\  am...|    0|[\\dm, for, link\\,...|[\\dm, link\\, , am...|(4096,[46,74,109,...|(4096,[46,74,109,...|[-1924.0534736504...|[1.0,9.3424150767...|       0.0|\n",
            "|a cool new room t...|    0|[a, cool, new, ro...|[cool, new, room,...|(4096,[353,384,69...|(4096,[353,384,69...|[-489.37373253025...|[0.99999572313602...|       0.0|\n",
            "|a couple of years...|    0|[a, couple, of, y...|[couple, years, a...|(4096,[43,362,419...|(4096,[43,362,419...|[-1331.4272246948...|[0.99999999301335...|       0.0|\n",
            "|a few close frien...|    0|[a, few, close, f...|[close, friends, ...|(4096,[109,122,15...|(4096,[109,122,15...|[-1995.3431809994...|[0.99999999994989...|       0.0|\n",
            "|a few years ago w...|    0|[a, few, years, a...|[years, ago, unde...|(4096,[72,99,103,...|(4096,[72,99,103,...|[-5112.7717113628...|[5.01923586582371...|       1.0|\n",
            "|a friend has just...|    0|[a, friend, has, ...|[friend, diagnose...|(4096,[55,459,485...|(4096,[55,459,485...|[-523.35520986503...|[0.99997640711375...|       0.0|\n",
            "|a little about me...|    1|[a, little, about...|[little, i’m, 26,...|(4096,[12,121,131...|(4096,[12,121,131...|[-2484.1509470227...|[0.99582710317807...|       0.0|\n",
            "|a little back sto...|    1|[a, little, back,...|[little, back, st...|(4096,[32,67,81,9...|(4096,[32,67,81,9...|[-6513.3024517593...|[3.27730404605765...|       1.0|\n",
            "|a little backstor...|    1|[a, little, backs...|[little, backstor...|(4096,[87,97,109,...|(4096,[87,97,109,...|[-3833.0681350281...|[1.36570081140872...|       1.0|\n",
            "|a little bit abou...|    1|[a, little, bit, ...|[little, bit, com...|(4096,[32,71,81,9...|(4096,[32,71,81,9...|[-6163.1233687744...|[0.97025102204100...|       0.0|\n",
            "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11OhIFABgYbl"
      },
      "source": [
        "# Binary Classification Evaluator\n",
        "\n",
        "\n",
        "eval4 = BinaryClassificationEvaluator(metricName=\"areaUnderROC\")\n",
        "print(\"Area Under the ROC Curve: {}\".format(eval1.evaluate(predictions2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nyFTXbJgbE6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69764a0f-7d37-4052-8232-f906043e5ac9"
      },
      "source": [
        "# Multiclass Classification Evaluator\n",
        "\n",
        "\n",
        "eval5 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "print(\"Accuracy: \" + str(eval5.evaluate(predictions2)))\n",
        "\n",
        "eval6 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
        "\n",
        "print(\"Precision: \" + str(eval6.evaluate(predictions2)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7768343909196532\n",
            "Precision: 0.7818824488930595\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEPc3UEcgifl"
      },
      "source": [
        "# Linear Support Vector Classification Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UciAXrl6hIkM"
      },
      "source": [
        "predictions3 = model3.transform(splits[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuqAhtz2hIY3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae958221-9b1f-439a-f5bb-b434c3d0a51d"
      },
      "source": [
        "predictions3.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|            selftext|label|               words|            filtered|         rawfeatures|            features|       rawPrediction|prediction|\n",
            "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|17 days now shes ...|    0|[17, days, now, s...|[17, days, shes, ...|(4096,[122,125,63...|(4096,[122,125,63...|[0.52678055893078...|       0.0|\n",
            "|175 high school s...|    0|[175, high, schoo...|[175, high, schoo...|(4096,[78,131,163...|(4096,[78,131,163...|[1.57196235216494...|       0.0|\n",
            "|21 year old male ...|    1|[21, year, old, m...|[21, year, old, m...|(4096,[32,109,120...|(4096,[32,109,120...|[-0.3517689099538...|       0.0|\n",
            "|22yo male and pre...|    1|[22yo, male, and,...|[22yo, male, pret...|(4096,[32,131,148...|(4096,[32,131,148...|[-2.9120081207204...|       1.0|\n",
            "|25 years old  - n...|    1|[25, years, old, ...|[25, years, old, ...|(4096,[16,18,32,5...|(4096,[16,18,32,5...|[-6.3693795564846...|       1.0|\n",
            "|27m posting with ...|    1|[27m, posting, wi...|[27m, posting, th...|(4096,[83,107,109...|(4096,[83,107,109...|[-1.4270902163680...|       1.0|\n",
            "|33m i have litera...|    1|[33m, i, have, li...|[33m, literally, ...|(4096,[237,379,40...|(4096,[237,379,40...|[-1.5918985571975...|       1.0|\n",
            "|4 years now since...|    1|[4, years, now, s...|[4, years, since,...|(4096,[122,144,17...|(4096,[122,144,17...|[-3.5236682611168...|       1.0|\n",
            "|5 months ago my l...|    0|[5, months, ago, ...|[5, months, ago, ...|(4096,[27,97,109,...|(4096,[27,97,109,...|[0.78394167397378...|       0.0|\n",
            "|5 years ago i spe...|    0|[5, years, ago, i...|[5, years, ago, s...|(4096,[51,68,131,...|(4096,[51,68,131,...|[2.46894752271759...|       0.0|\n",
            "|\\dm for link\\  am...|    0|[\\dm, for, link\\,...|[\\dm, link\\, , am...|(4096,[46,74,109,...|(4096,[46,74,109,...|[5.67618026274329...|       0.0|\n",
            "|a cool new room t...|    0|[a, cool, new, ro...|[cool, new, room,...|(4096,[353,384,69...|(4096,[353,384,69...|[2.09954508651554...|       0.0|\n",
            "|a couple of years...|    0|[a, couple, of, y...|[couple, years, a...|(4096,[43,362,419...|(4096,[43,362,419...|[3.00731784295768...|       0.0|\n",
            "|a few close frien...|    0|[a, few, close, f...|[close, friends, ...|(4096,[109,122,15...|(4096,[109,122,15...|[1.00837488940744...|       0.0|\n",
            "|a few years ago w...|    0|[a, few, years, a...|[years, ago, unde...|(4096,[72,99,103,...|(4096,[72,99,103,...|[2.68155047577111...|       0.0|\n",
            "|a friend has just...|    0|[a, friend, has, ...|[friend, diagnose...|(4096,[55,459,485...|(4096,[55,459,485...|[2.35708393764515...|       0.0|\n",
            "|a little about me...|    1|[a, little, about...|[little, i’m, 26,...|(4096,[12,121,131...|(4096,[12,121,131...|[-2.2318172620235...|       1.0|\n",
            "|a little back sto...|    1|[a, little, back,...|[little, back, st...|(4096,[32,67,81,9...|(4096,[32,67,81,9...|[-4.8805432698607...|       1.0|\n",
            "|a little backstor...|    1|[a, little, backs...|[little, backstor...|(4096,[87,97,109,...|(4096,[87,97,109,...|[-6.8903395260669...|       1.0|\n",
            "|a little bit abou...|    1|[a, little, bit, ...|[little, bit, com...|(4096,[32,71,81,9...|(4096,[32,71,81,9...|[-1.1247744143862...|       1.0|\n",
            "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_ywLusRgnSy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80aff751-802a-46dd-826f-236d41a3c353"
      },
      "source": [
        "# Binary Classification Evaluator\n",
        "\n",
        "\n",
        "eval7 = BinaryClassificationEvaluator(metricName=\"areaUnderROC\")\n",
        "print(\"Area Under the ROC Curve: {}\".format(eval7.evaluate(predictions3)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Area Under the ROC Curve: 0.9283388771784026\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SPKaoZwguHA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edf23750-e36b-4420-ce26-86f8d55ed64d"
      },
      "source": [
        "# Multiclass Classification Evaluator\n",
        "\n",
        "\n",
        "eval8 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "print(\"Accuracy: \" + str(eval8.evaluate(predictions3)))\n",
        "\n",
        "eval9 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
        "\n",
        "print(\"Precision: \" + str(eval9.evaluate(predictions3)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8428814013763518\n",
            "Precision: 0.8619162777668492\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FcWX9lMeYx3"
      },
      "source": [
        "# Saving the Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0seDEUAecrb"
      },
      "source": [
        "# Save our Logistic Regression Model\n",
        "model1.save('/content/drive/MyDrive/490/Model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bnmp0Av2KHJV"
      },
      "source": [
        "#Resources\n",
        "\n",
        "https://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression\n",
        "\n",
        "https://spark.apache.org/docs/1.5.1/mllib-naive-bayes.html\n",
        "\n",
        "https://stackoverflow.com/questions/44779002/pyspark-to-pmml-field-label-does-not-exist-error\n",
        "\n",
        "https://stackoverflow.com/questions/47707405/spark-logistic-regression-for-binary-classification-apply-new-threshold-for-pre\n",
        "\n",
        "https://stackoverflow.com/questions/57716806/split-spark-dataframe-in-half-without-overlapping-data\n",
        "\n",
        "https://hyukjin-spark.readthedocs.io/en/latest/reference/api/pyspark.sql.DataFrame.randomSplit.html\n",
        "\n",
        "https://stackoverflow.com/questions/40163106/cannot-find-col-function-in-pyspark\n",
        "\n",
        "https://spark.apache.org/docs/1.6.1/ml-guide.html\n",
        "\n",
        "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\n",
        "\n"
      ]
    }
  ]
}